---
title: httrack
created: 2024-04-26T05:20:24+0800
updated: 2024-04-26T05:20:24+0800
---


[httrack](https://www.httrack.com/) 是用来爬站的工具。它提供命令行和图形化界面的程序。

## 我的最佳实践

```sh
httrack <target_url> \
        -O ./ -w -v -i -A200000 -%c5 -c4 -T120 -R3 -N0 -K0 -%x -%T -s0 -%k -B -e \
        -F "<user_agent>" \
        -%R "<refer_url>" \
        -* \
        +<allow_domain1>/* \
        +<allow_domain2>/*
```

- `<>` 括号里的填实际的值。
- 超时：`-R3` 重试 3 次，`-T120` 每个链接的超时时间 120 秒。
- 限速：`-%c5` 每秒最大链接数是 5 个。`-c4` 总共 4 个并发链接。`-A200000` 限速 200 KB/s。这些限制都是为了避免把目标网站搞挂。
- `-F` 给每个请求加上 user-agent 信息。用来伪装成真实用户的请求。
- `-%R` 给每个请求加上 refer 地址，不加这个参数也行。用来伪装成真实用户的请求。
- `-*` 表示禁止抓所有的 url，后跟 `+` 来选择抓取哪些 url。`-` 和 `+` 的先后顺序有要求，后面的权重高于前面的，详见 httrack 文档。
  - 因为大多数目标网址会引用其他域名的网址，由于我用了 `-B` 和 `-e` 参数，会导致全部抓取。如果不加 `-B` 和 `-e` 参数，很多存在别的域名下的资源文件就不会抓取。
- 可按需要加上 `--update` 参数。
- 其他参数也很重要，由于不需要填参数，我就懒得解释。
- **建议执行命令的时间选在网站访问低峰，比如别人都睡觉的时候。免得你误操作发出大量并发请求搞挂某些不经折腾的网站。**

**还存在问题**：有个别地方虽然抓取了网页，但没有转换成相对链接，依然还是原链接。总之先记录下来，以后再说。
